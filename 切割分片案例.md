#### 对文档切块分割

     1.清洗数据
        删除文件中的英文和/

        ```
            import re

            def remove_english(input_file, output_file):
                """
                去除文件中所有英文字符并生成新文件
                :param input_file: 输入文件路径
                :param output_file: 输出文件路径
                """
                try:
                    with open(input_file, 'r', encoding='utf-8') as f_in:
                        content = f_in.read()
            
                    # 使用正则表达式移除所有英文字母
                    filtered_content = re.sub('[A-Za-z/]', '', content)
            
                    with open(output_file, 'w', encoding='utf-8') as f_out:
                        f_out.write(filtered_content)
                        
                    print(f"处理完成，已生成新文件：{output_file}")
                    
                except Exception as e:
                    print(f"处理出错：{str(e)}")
            
            # 使用示例
            # remove_english('./data/demo-2.txt', './data/demo-2-1.txt')
            
        ```

    2.对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引
      index = VectorStoreIndex.from_documents(documents, transformations=[SentenceSplitter(chunk_size=256)])

      SentenceSplitter 参数详细设置：

          预设会以 1024 个 token 为界切割片段, 每个片段的开头重叠上一个片段的 200 个 token 的内容。

          ```properties
          
                chunk_size = 1024,    # 切片 token 数限制
                chunk_overlap = 200,  # 切片开头与前一片段尾端的重复 token 数
                paragraph_separator = '\n\n\n', # 段落的分界
                secondary_chunking_regex = '[^,.;。？！]+[,.;。？！]?' # 单一句子的样式
                separator = ' ', # 最小切割的分界字元
                
          ```

    3.读取文档, 构建向量索引
      documents = SimpleDirectoryReader("./data", required_exts=[".txt"]).load_data() 
      index = VectorStoreIndex.from_documents(documents, transformations=[SentenceSplitter(chunk_size=256)])

    4.向量存储
      # 将embedding向量和向量索引存储到文件中
      # ./doc_emb 是存储路径
      index.storage_context.persist(persist_dir='./doc_emb')

    5.从向量数据库检索
     
      将embedding向量和向量索引存储到文件中后，我们就不需要重复地执行对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引的操作了。

      以下代码演示了如何使用LlamaIndex读取结构化文件中的embedding向量和向量索引数据：

      # 从存储文件中读取embedding向量和向量索引
      storage_context = StorageContext.from_defaults(persist_dir="./doc_emb")

      # 根据存储的embedding向量和向量索引重新 构建检索索引
      index = load_index_from_storage(storage_context)

      # 构建查询引擎
      query_engine = index.as_query_engine(streaming=True, similarity_top_k=5)

      # 查询获得答案
      response = query_engine.query("不耐疲劳，口燥、咽干可能是哪些证候？")
      response.print_response_stream()
      print()


   6.追踪哪些文档片段被检索
    
      # 从存储文件中读取embedding向量和向量索引
      storage_context = StorageContext.from_defaults(persist_dir="./doc_emb")
     
      # 根据存储的embedding向量和向量索引重新构建检索索引
      index = load_index_from_storage(storage_context)
     
      # 构建查询引擎
      query_engine = index.as_query_engine(similarity_top_k=5)
     
      # 获取我们抽取出的相似度 top 5 的片段
      contexts = query_engine.retrieve(QueryBundle("不耐疲劳，口燥、咽干可能是哪些证候？"))
      print('-' * 10 + 'ref' + '-' * 10)
      for i, context in enumerate(contexts):
          print('#' * 10 + f'chunk {i} start' + '#' * 10)
          content = context.node.get_content(metadata_mode=MetadataMode.LLM)
          print(content)
          print('#' * 10 + f'chunk {i} end' + '#' * 10)
      print('-' * 10 + 'ref' + '-' * 10)
     
      # 查询获得答案
      response = query_engine.query("不耐疲劳，口燥、咽干可能是哪些证候？")
      print(response)
