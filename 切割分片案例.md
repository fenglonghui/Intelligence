#### 对文档切块分割

#####  文档转换
         使用MinerU 对文档进行转换
         MinerU在线转换地址: https://mineru.net/OpenSourceTools/Extractor

#####  1.清洗数据
        删除文件中的英文和/

        ```
            import re

            def remove_english(input_file, output_file):
                """
                去除文件中所有英文字符并生成新文件
                :param input_file: 输入文件路径
                :param output_file: 输出文件路径
                """
                try:
                    with open(input_file, 'r', encoding='utf-8') as f_in:
                        content = f_in.read()
            
                    # 使用正则表达式移除所有英文字母
                    filtered_content = re.sub('[A-Za-z/]', '', content)
            
                    with open(output_file, 'w', encoding='utf-8') as f_out:
                        f_out.write(filtered_content)
                        
                    print(f"处理完成，已生成新文件：{output_file}")
                    
                except Exception as e:
                    print(f"处理出错：{str(e)}")
            
            # 使用示例
            # remove_english('./data/demo-2.txt', './data/demo-2-1.txt')
            
        ```

#####  2.对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引
         index = VectorStoreIndex.from_documents(documents, transformations=[SentenceSplitter(chunk_size=256)])

         SentenceSplitter 参数详细设置：

            预设会以 1024 个 token 为界切割片段, 每个片段的开头重叠上一个片段的 200 个 token 的内容。

            ```properties
          
                chunk_size = 1024,    # 切片 token 数限制
                chunk_overlap = 200,  # 切片开头与前一片段尾端的重复 token 数
                paragraph_separator = '\n\n\n', # 段落的分界
                secondary_chunking_regex = '[^,.;。？！]+[,.;。？！]?' # 单一句子的样式
                separator = ' ', # 最小切割的分界字元
                
           ```

#####  3.读取文档, 构建向量索引
         documents = SimpleDirectoryReader("./data", required_exts=[".txt"]).load_data() 
         index = VectorStoreIndex.from_documents(documents, transformations=[SentenceSplitter(chunk_size=256)])

#####  4.向量存储
         # 将embedding向量和向量索引存储到文件中
         # ./doc_emb 是存储路径
         index.storage_context.persist(persist_dir='./doc_emb')

#####  5.从向量数据库检索
     
         将embedding向量和向量索引存储到文件中后，我们就不需要重复地执行对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引的操作了。

         以下代码演示了如何使用LlamaIndex读取结构化文件中的embedding向量和向量索引数据：

         # 从存储文件中读取embedding向量和向量索引
         storage_context = StorageContext.from_defaults(persist_dir="./doc_emb")

         # 根据存储的embedding向量和向量索引重新 构建检索索引
         index = load_index_from_storage(storage_context)

         # 构建查询引擎
         query_engine = index.as_query_engine(streaming=True, similarity_top_k=5)

         # 查询获得答案
         response = query_engine.query("不耐疲劳，口燥、咽干可能是哪些证候？")
         response.print_response_stream()
         print()


#####  6.追踪哪些文档片段被检索
    
         # 从存储文件中读取embedding向量和向量索引
         storage_context = StorageContext.from_defaults(persist_dir="./doc_emb")
     
         # 根据存储的embedding向量和向量索引重新构建检索索引
         index = load_index_from_storage(storage_context)
     
         # 构建查询引擎
         query_engine = index.as_query_engine(similarity_top_k=5)
     
         # 获取我们抽取出的相似度 top 5 的片段
         contexts = query_engine.retrieve(QueryBundle("不耐疲劳，口燥、咽干可能是哪些证候？"))
         print('-' * 10 + 'ref' + '-' * 10)
         for i, context in enumerate(contexts):
             print('#' * 10 + f'chunk {i} start' + '#' * 10)
             content = context.node.get_content(metadata_mode=MetadataMode.LLM)
             print(content)
             print('#' * 10 + f'chunk {i} end' + '#' * 10)
         print('-' * 10 + 'ref' + '-' * 10)
     
         # 查询获得答案
         response = query_engine.query("不耐疲劳，口燥、咽干可能是哪些证候？")
         print(response)


#####  7.RAG 检索底层实现细节及分析
         知道了如何追踪哪些文档片段被用于检索增强生成，但我们仍不知道RAG过程中到底发生了什么，为什么大模型能够根据检索出的文档片段进行回复？

       ```
               import logging
               import sys
               import os
               from llama_index.core import PromptTemplate, Settings, StorageContext, load_index_from_storage
               from llama_index.core.callbacks import LlamaDebugHandler, CallbackManager
               from llama_index.embeddings.huggingface import HuggingFaceEmbedding
               from llama_index.llms.huggingface import HuggingFaceLLM
               
               # 定义日志
               logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
               logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))
               
               
               # 定义system prompt
               SYSTEM_PROMPT = """You are a helpful AI assistant."""
               query_wrapper_prompt = PromptTemplate(
                   "[INST]<<SYS>>\n" + SYSTEM_PROMPT + "<</SYS>>\n\n{query_str}[/INST] "
               )
               
               # 使用llama-index创建本地大模型
               Settings.llm = HuggingFaceLLM(
                   context_window = 4096,
                   max_new_tokens = 2048,
                   generate_kwargs = {"temperature": 0.0, "do_sample": False},
                   query_wrapper_prompt = query_wrapper_prompt,
                   tokenizer_name = "/home/kevin/projects/models/Qwen/Qwen1.5-7B-Chat",
                   model_name = "/home/kevin/projects/models/Qwen/Qwen1.5-7B-Chat",
                   device_map = "auto",
                   model_kwargs = {"torch_dtype": torch.float16},
               )
               
               # 使用LlamaDebugHandler构建事件回溯器，以追踪LlamaIndex执行过程中发生的事件
               llama_debug = LlamaDebugHandler(print_trace_on_end=True)
               callback_manager = CallbackManager([llama_debug])
               Settings.callback_manager = callback_manager
               
               # 使用llama-index-embeddings-huggingface构建本地embedding模型
               Settings.embed_model = HuggingFaceEmbedding(
                   model_name = "/home/kevin/projects/models/BAAI/bge-base-zh-v1.5"
               )
               
               # 从存储文件中读取embedding向量和向量索引
               storage_context = StorageContext.from_defaults(persist_dir="./doc_emb")
               
               # 根据存储的embedding向量和向量索引重新构建检索索引
               index = load_index_from_storage(storage_context)
               
               # 构建查询引擎
               query_engine = index.as_query_engine(similarity_top_k=5)
               
               # 查询获得答案
               response = query_engine.query("不耐疲劳，口燥、咽干可能是哪些证候？")
               print(response)
               
               # get_llm_inputs_outputs 返回每个LLM调用的开始/结束事件
               event_pairs = llama_debug.get_llm_inputs_outputs()
               
               # print(event_pairs[0][1].payload.keys()) # 输出事件结束时所有相关的属性
               
               # 输出 Promt 构建过程
               print(event_pairs[0][1].payload["formatted_prompt"])
          
       ```

      Query 过程底层分析:
          **********
          Trace: query
              |_query -> 63.648696 seconds
                |_retrieve -> 1.186543 seconds
                  |_embedding -> 1.047233 seconds
                |_synthesize -> 62.461404 seconds
                  |_templating -> 3.3e-05 seconds
                  |_llm -> 62.451146 seconds
          **********
          </pre>

          以上的输出记录了query在程序过程中经历的阶段和所用的时间，整个过程分为两个阶段：

            - 抽取（retrieve）
            - 合成（synthesize）。
          
          合成阶段的templating步骤会将query和抽取出来的文档片段组合成模板，构成新的query，然后调用LLM，得到最终的response。
          
          所以，只要找到templating所构建的新query，就可以知道为什么大模型能够根据我们检索出来的文档进行回复了。


##### 8.Retrieve 检索进阶
        抽取（retrieve）阶段的retrievers模块规定了针对查询从知识库获取相关上下文的技术。我们之前使用的都是默认的方法，
        
        其实LlamaIndex官方为我们提供了一些其他常用的方法：
                  
           - SimilarityPostprocessor: 使用similarity_cutoff设置阈值。移除低于某个相似度分数的节点。
           - KeywordNodePostprocessor: 使用required_keywords和exclude_keywords。根据关键字包含或排除过滤节点。
           - MetadataReplacementPostProcessor: 用其元数据中的数据替换节点内容。
           - LongContextReorder: 重新排序节点，这有利于需要大量顶级结果的情况，可以解决模型在扩展上下文中的困难。
           - SentenceEmbeddingOptimizer: 选择percentile_cutoff或threshold_cutoff作为相关性。基于嵌入删除不相关的句子。
           - CohereRerank: 使用coherence ReRank对节点重新排序，返回前N个结果。
           - SentenceTransformerRerank: 使用SentenceTransformer交叉编码器对节点重新排序，产生前N个节点。
           - LLMRerank: 使用LLM对节点重新排序，为每个节点提供相关性评分。
           - FixedRecencyPostprocessor: 返回按日期排序的节点。
           - EmbeddingRecencyPostprocessor: 按日期对节点进行排序，但也会根据嵌入相似度删除较旧的相似节点。
           - TimeWeightedPostprocessor: 对节点重新排序，偏向于最近未返回的信息。
           - PIINodePostprocessor(β): 可以利用本地LLM或NER模型删除个人身份信息。
           - PrevNextNodePostprocessor(β): 根据节点关系，按顺序检索在节点之前、之后或两者同时出现的节点


##### 9.响应合成器 response synthesize
        合成（synthesize）阶段的响应合成器（response synthesizer）会引导LLM生成响应，将用户查询与检索到的文本块混合在一起，并给出一个精心设计的答案。

        LlamaIndex官方为我们提供了多种响应合成器：
         
           - Refine: 这种方法遍历每一段文本，一点一点地精炼答案。
           - Compact: 是Refine的精简版。它将文本集中在一起，因此需要处理的步骤更少。
           - Tree Summarize: 想象一下，把许多小的答案结合起来，再总结，直到你得到一个主要的答案。
           - Simple Summarize: 只是把文本片段剪短，然后给出一个快速的总结。
           - No Text: 这个问题不会给你答案，但会告诉你它会使用哪些文本。
           - Accumulate: 为每一篇文章找一堆小答案，然后把它们粘在一起。
           - Compact Accumulate: 是“Compact”和“Accumulate”的合成词。
         
        现在，我们选择一种retriever和一种response synthesizer。retriever选择SimilarityPostprocessor，response synthesizer选择Refine
          
          
