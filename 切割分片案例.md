#### 对文档切块分割

     1.清洗数据
        删除文件中的英文和/

        ```
            import re

            def remove_english(input_file, output_file):
                """
                去除文件中所有英文字符并生成新文件
                :param input_file: 输入文件路径
                :param output_file: 输出文件路径
                """
                try:
                    with open(input_file, 'r', encoding='utf-8') as f_in:
                        content = f_in.read()
            
                    # 使用正则表达式移除所有英文字母
                    filtered_content = re.sub('[A-Za-z/]', '', content)
            
                    with open(output_file, 'w', encoding='utf-8') as f_out:
                        f_out.write(filtered_content)
                        
                    print(f"处理完成，已生成新文件：{output_file}")
                    
                except Exception as e:
                    print(f"处理出错：{str(e)}")
            
            # 使用示例
            # remove_english('./data/demo-2.txt', './data/demo-2-1.txt')
            
        ```

    2.对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引
      index = VectorStoreIndex.from_documents(documents, transformations=[SentenceSplitter(chunk_size=256)])

      SentenceSplitter 参数详细设置：

          预设会以 1024 个 token 为界切割片段, 每个片段的开头重叠上一个片段的 200 个 token 的内容。

          ```properties
          
                chunk_size = 1024,    # 切片 token 数限制
                chunk_overlap = 200,  # 切片开头与前一片段尾端的重复 token 数
                paragraph_separator = '\n\n\n', # 段落的分界
                secondary_chunking_regex = '[^,.;。？！]+[,.;。？！]?' # 单一句子的样式
                separator = ' ', # 最小切割的分界字元
                
          ```

          
