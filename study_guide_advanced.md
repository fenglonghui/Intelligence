### 学习导读高级指引

#### 1.模型平台, 1.modelscope 平台, 2.Huggingface 平台

#### 2.提示词工程
       对于智能强的模型, 模型会按照提示词格式严格输出内容, 对于模型能力一般货比较弱的模型, 提示词格式几乎是失效!!!

#### 3.数据分割技术
       1.数据拆分类型:
          1.按固定长度分割
          2.语义分割
          3.按业务需求分割    (推荐)

       2.智能文档解析技术
          使用Mineru做文档转换,转换为markdown文件, 然后线下对markdown文件进行人工处理 (推荐) 
              https://mineru.net/
          使用 deepseek-ocr 处理复杂的多模态的pdf文档

#### 4.RAG知识库-数据结构化处理
       按业务需求对数据进行分割整理

#### 5.DeepSeek OCR工具(企业级vLLM本地部署)及Mineru工具数据解析
       用于数据文档解析

#### 6.Llama_indexRAG进阶_文档切分与重排序
       基于RAG初次排序基础之上, 对数据进行重排序(rerank)以提高精度、准确率

       使用重排序模型, 将top_k检索结果的语义score明显的分割开来: 使得语义相近的score分数特别相大, 语义不相关的score分数特别小, 即分割界限极大明显, 以此提高检索精度

#### 6.1 改进索引算法
         知识图谱: 利用知识图谱中的语义信息和实体关系, 增强对查询和文档的理解, 提升召回的相关性

#### 6.2 关键词抽取技术: TF-IDF, TextRank, 可以提取关键词
         BM25召回

#### 6.3 Small-to-Big 索引策略
        一种高效的检索方法, 适用于处理长文档或多文档场景
        
        small-to-big 机制:
              1.小规模内容检索
              2.链接到大规模内容
              3.上下文补充

        场景: 提供一百多篇文档, 给每个文档提取摘要或者说提取关键的句子, 然后通过摘要或关键句子的段落, 再来链接到相关文档

        实现原理:
            1.文档 -> 生成摘要 (LLM + Prompt)
               原始文档文件夹 -> 向量数据库 faiss-1
               摘要文档文件夹 -> 向量数据库 faiss-2
               两者建立1v1对应关系

            2.small-to-big 检索策略
              query -> faiss-2 -> faiss-1

#### 7.查询改写(RAG优化检索效果)
       用户提出的问题目的不清楚(意图模糊), 可通过识别用户意图,对用户问题进行改写,从而提高LLM用户问题回答的效果
       解决方法: 通过查询转换明确用户意图

       实现步骤:
              意图识别: 你用自然语言处理技术识别用户意图 (使用大模型)
              查询扩展:根据识别结果扩展查询
              检索: 使用扩展后的查询检索相关文档

       具体实施: 写个提示词, 通过大模型来重新生成, 连重新理解用户意图, 再来做检索查询

       多路召回方案(相似语义改写)
              使用大模型, 将用户查询改写成多个语义相近的查询, 提升召回多样性
              LangChain的MultiQueryRetriever支持多查询召回, 再进行回答问题
              LLamaIndex的RAGfusion 支持多查询召回, 再进行回答问题

       查询改写代码: 
       ```
              from langchain.retrievers import MultiQueryRetriever
       
              # 创建MultiQueryRetriever
              retriever = MultiQueryRetriever.from_llm(
                  retriever=vectorstore.as_retriever(),
                  llm=llm
              )
              
              # 示例查询
              query = "客户经理的考核标准是什么？"
              # 执行查询
              results = retriever.get_relevant_documents(query)
              
              # 打印结果
              print(f"查询: {query}")
              print(f"找到 {len(results)} 个相关文档:")
              for i, doc in enumerate(results):
                  print(f"\n文档 {i+1}:")
                  print(doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content)
       ```

#### 7.1 双向改写 (RAG优化检索效果)
         将查询改写成文档(Query2Doc) 或为文档生成查询 (Doc2Query), 缓解短文本向量化效果差的问题
         1.将查询改写成文档
         2.为文档生成查询

#### 7.2 混合检索 (RAG优化检索效果)
         就是根据语义检索和关键字检索进行查询, 会降低检索性能,提高检索精度

#### 8.多场景多领域RAG知识隔离架构
       多个知识库统一调用管里架构
      
#### 9.大模型微调 
       目标: 基于现有的私有数据, 微调可以让模型具备处理该数据的功能
       微调落地场景: 
           1.模型固有的信息变换
           2.对话风格 
           3.针对专业问答系统的问题理解不到位时, 会使用微调技术帮助模型更好的理解用户的问题

       微调的分类:
           1.全量微调, (相当于预训练)
           2.增量微调, (冻结模型已有参数, 增加微调网络结构进行微调, 比如: 基于BERT模型的自定义微调训练, 用来进行分类)
           3.局部微调 (比如: RAG微调)

#### 10.自定义微调训练BERT模型效果测试

#### 11.GPT2中文生成模型定制化微调训练

#### 12.大模型本地化部署
       1. Ollama 服务, 适用于本地私优化部署使用
       2. vLLM 服务,    高并发性能突出, 商业上推荐它
       3. LMDeploy 服务,  速度快, 更加适合国产设备

#### 13.大模型微调-LLama Factor微调Qwen

#### 14.Ollama部署微调大模型

#### 15.LLamaFactory 模型导出量化

#### 16.vLLM自定义对话模板

#### 17.LLamaFactory与Xtuner分布式微调大模型

#### 18.大模型压缩训练（知识蒸馏）
        该技术不属于微调范畴, 适用于大模型初创研发领域

#### 19.大模型微调项目实战-数据工程篇

#### 20.大模型微调项目实战-训练与部署

#### 21.大模型评估测试 OpenCompass

#### 22.大模型分布式推理与量化部署

#### 23.多模态大模型应用

#### 24.RAG+微调实现智能专家系统（方案数据篇）

#### 25.RAG+微调实现智能专家系统（部署测试）

#### PDF 文档解析难点、痛点
     图片、表格、公式 转换为Text 文本  

     可使用 1.DeepSeek OCR工具 解析
           2.Mineru 转换解析
      
     不一定解析转换的相当满意

#### 未来展望
       多智能体的协同工作,能真正实现人工智能化
