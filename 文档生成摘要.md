#### 1.è¯»å–pdfæ–‡ä»¶å†…å®¹, æå–æ¯æ®µçš„æ–‡æ¡£å†…å®¹æ‘˜è¦, å¦‚é¥¿è™å®ç°?

æ ¸å¿ƒæ€è·¯ï¼š
PyMuPDF åˆ‡å— â†’ å¥å‘é‡å»é‡ â†’ ä¸­æ–‡ LLM é€æ®µå‹ç¼© â†’ å¼‚æ­¥æ‰¹å¤„ç†ï¼Œåƒé¥¿è™ä¸€æ ·â€œä¸€å£ä¸€å—â€ã€‚

1.ä¸€æ¬¡æ€§è£…å¥½ä¾èµ–
  pip install pymupdf sentence-transformers torch transformers tqdm

2.é¥¿è™è„šæœ¬ï¼ˆsave as tiger_parse.pyï¼‰
```
      import fitz, torch, asyncio, json
      from transformers import AutoTokenizer, AutoModelForCausalLM
      from sentence_transformers import SentenceTransformer
      from tqdm.asyncio import tqdm_asyncio


      PDF  = "demo.pdf"
      MAX_TOKENS = 120       # å•æ®µæ‘˜è¦ä¸Šé™
      BATCH_SZ   = 8         # å¹¶å‘æ•°ï¼Œæ˜¾å­˜å°å°±è°ƒä½
      DEVICE     = "cuda" if torch.cuda.is_available() else "cpu"


      # 1. è½»é‡ä¸­æ–‡LLMï¼šIDEA çš„ 1.3B æ¨¡å‹ï¼ŒCPU ä¹Ÿèƒ½è·‘
      tok = AutoTokenizer.from_pretrained("IDEA-CCNL/Randeng-1.3B-Chat")
      llm = AutoModelForCausalLM.from_pretrained("IDEA-CCNL/Randeng-1.3B-Chat",
                                                 torch_dtype=torch.float16 if DEVICE=="cuda" else torch.float32,
                                                 device_map="auto")
      llm.eval()
      
      # 2. å¥å‘é‡å»é‡ç”¨ï¼ˆé¿å…æ ‡é¢˜/é¡µçœ‰é‡å¤ï¼‰
      sim = SentenceTransformer("shibing624/text2vec-base-chinese")
      
      def split_paragraphs(path):
          """æŒ‰æ¢è¡Œ+é•¿åº¦>30 æ‹†æ®µï¼ŒåŒæ—¶å»é‡"""
          doc, seen, paras = fitz.open(path), set(), []
          for page in doc:
              blocks = page.get_text("blocks")        # (x0,y0,x1,y1,text,block_no)
              for b in blocks:
                  txt = b[4].strip().replace(" ","")
                  if len(txt)<30: continue
                  emb = sim.encode(txt, normalize_embeddings=True)
                  key = tuple(emb.round(3).tolist())
                  if key in seen: continue
                  seen.add(key)
                  paras.append(txt)
          return paras
      
      async def summarize(chunk):
          prompt = f"ç”¨ä¸€å¥è¯æ€»ç»“ä¸‹æ–‡ï¼š\n{chunk}\næ‘˜è¦ï¼š"
          inputs = tok(prompt, return_tensors="pt").to(llm.device)
          with torch.no_grad():
              out = llm.generate(**inputs,
                                 max_new_tokens=MAX_TOKENS,
                                 do_sample=False,
                                 eos_token_id=tok.eos_token_id)
          return tok.decode(out[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True).strip()
      
      async def main():
          paragraphs = split_paragraphs(PDF)
          results = await tqdm_asyncio.gather(*[summarize(p) for p in paragraphs], total=len(paragraphs))
          dump = [{"paragraph": p, "summary": s} for p, s in zip(paragraphs, results)]
          json.dump(dump, open("tiger_out.json","w",encoding="utf-8"), ensure_ascii=False, indent=2)
          print("ğŸ… å®Œæˆï¼å·²ç”Ÿæˆ tiger_out.json")
    
    if __name__ == "__main__":
        asyncio.run(main())

```
è¾“å‡ºæ•°æ®æ ¼å¼:
            [
            {
                "paragraph": "éšç€æ·±åº¦å­¦ä¹ çš„çˆ†å‘ï¼ŒGPU å¹¶è¡Œè®¡ç®—æˆä¸ºä¸»æµâ€¦â€¦",
                "summary": "æ·±åº¦å­¦ä¹ å…´èµ·æ¨åŠ¨ GPU å¹¶è¡Œè®¡ç®—æˆä¸ºä¸»æµã€‚"
              },
              ...
            ]


æƒ³å†ç‹ ä¸€ç‚¹
    ä½¿ç”¨æ›´å¤§æ¨¡å‹ï¼šæŠŠ llm æ¢æˆ ChatGLM3-6Bã€Qwen-14Bï¼Œæ‘˜è¦è´¨é‡æ›´é«˜ã€‚
    é€Ÿåº¦ç¿»å€ï¼šæŠŠ llm æ”¹æˆ INT4 é‡åŒ– + vLLM æ¨ç†ï¼Œæ‰¹å¤„ç† 32 å¹¶å‘ã€‚
    æ®µè½æ›´ç»†ï¼šç”¨ re.split(r'\n\s*\n', text) å†æ‹†å°ï¼Œæ‘˜è¦ç²’åº¦æ›´ç»†ã€‚

è¿™æ¡â€œé¥¿è™â€è„šæœ¬é›¶é…ç½®ã€ä¸ä¾èµ–äº‘ç«¯ï¼Œæœ¬åœ° GPU/CPU éƒ½èƒ½è·‘ï¼Œæ‹¿è¿‡å»å°±èƒ½å•ƒ PDF


#### 2.åŒå‘ä¿®æ”¹å®ç°, æå‡Retrieverè®­ç»ƒæ•ˆæœ
       
       åŒå‘æ”¹å†™ä»·å€¼å¯¹ç…§è¡¨:
       
       | åŸæœ‰é—®é¢˜             |         åŒå‘æ”¹å†™è§£å†³æ–¹æ¡ˆ                                     |
       | --------------      | --------------------------------------------------------- |
       | æŸ¥è¯¢å¤ªçŸ­ï¼Œéš¾ä»¥è¡¨è¾¾æ„å›¾  |   Query2Docï¼šæŠŠçŸ­ query æ‰©å±•æˆä¸€æ®µâ€œç†æƒ³ç­”æ¡ˆâ€ï¼Œè¯­ä¹‰æ›´ä¸°å¯Œ        |
       | æ–‡æ¡£å¤ªé•¿ï¼Œä¿¡æ¯å¯†åº¦è¿‡é«˜  |   Doc2Queryï¼šä¸ºé•¿æ–‡ç”Ÿæˆå¤šä¸ªå‡é—®é¢˜ï¼Œå»ºç«‹æ›´å¤šæ£€ç´¢å…¥å£              |
       | å‘é‡åŒ¹é…å¤±è´¥          |   æ”¹å†™åçš„ query/doc è¡¨è¿°æ›´å®Œæ•´ï¼Œembedding ç›¸ä¼¼åº¦æ˜¾è‘—æå‡       |
       | æ•°æ®æ„é€ å›°éš¾          |   Doc2Query å¯ç¦»çº¿è‡ªåŠ¨ç”Ÿæˆ\<query, doc>æ­£ä¾‹ï¼Œé™ä½äººå·¥æ ‡æ³¨æˆæœ¬   |

       
      å…·ä½“å®ç°(é€šè¿‡é‡å†™Prompt, è®©LLMæ¥å®ç°):

          1.Query2Doc ç¤ºä¾‹ä»£ç ï¼ˆLangChain + LLMï¼‰ï¼š
          
              query = "å¦‚ä½•æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Ÿ"
              prompt = f"å°†ä»¥ä¸‹ç®€çŸ­é—®é¢˜æ‰©å†™ä¸ºä¸€æ®µæŠ€æœ¯æ–‡æ¡£è¯´æ˜ï¼š\n\né—®é¢˜ï¼š{query}\n\næ‰©å†™ï¼š"
              response = llm.invoke(prompt)
              print(response)


          2.Doc2Query ç¤ºä¾‹ä»£ç ï¼ˆLangChain + LLMï¼‰ï¼š
          
              doc = "æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒå’Œåˆ†å¸ƒå¼æŠ€æœ¯æå‡æ·±åº¦å­¦ä¹ è®­ç»ƒæ•ˆç‡çš„æ–¹æ³•ã€‚"
              prompt = f"æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆ5ä¸ªç”¨æˆ·å¯èƒ½æå‡ºçš„é—®é¢˜ï¼š\n\næ–‡æ¡£ï¼š{doc}\n\né—®é¢˜ï¼š"
              response = llm.invoke(prompt)
              print(response)
