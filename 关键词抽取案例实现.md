### RAG 关键词提取案例

    在 RAG 系统里，关键词提取通常承担“召回”或“重排序”的角色：先用轻量级算法（TF-IDF、TextRank、jieba 等）离线给知识库切片打标，
    再在线上把用户 query 同样抽成关键词，用集合/倒排/向量做初筛或粗排，最后交给向量检索或 LLM 精排。
    
    下面给出 3 套可直接落地的 Python 实现，按“轻量→中等→复杂” 递进示例:

####  1.极简版：jieba-TF-IDF 倒排召回（适合中文 FAQ）

      ```
            # pip install jieba
            import jieba.analyse
            
            class JiebaKeywordRAG:
                def __init__(self, docs: list[str], top_k=5):
                    self.docs = docs
                    # 离线建倒排
                    self.doc_keywords = [set(jieba.analyse.extract_tags(d, topK=10)) for d in docs]
                    self.top_k = top_k
            
                def search(self, query: str):
                    query_kw = set(jieba.analyse.extract_tags(query, topK=10))
                    scores = [(i, len(query_kw & kw)) for i, kw in enumerate(self.doc_keywords)]
                    scores = sorted(scores, key=lambda x: x[1], reverse=True)[:self.top_k]
                    return [self.docs[i] for i, _ in scores if _ > 0]
            
            # demo
            docs = ["顺丰快递如何寄件", "圆通价格查询", "京东物流时效"]
            rag = JiebaKeywordRAG(docs)
            print(rag.search("想发顺丰"))   # -> ['顺丰快递如何寄件']

      ```
      特点：零依赖、毫秒级；缺点是语义能力弱，适合垂直 FAQ


####  2.进阶版：关键词 + 向量混合重排序

      ```
            # pip install sentence-transformers scikit-learn
            
            from sentence_transformers import SentenceTransformer
            from sklearn.feature_extraction.text import TfidfVectorizer
            import numpy as np
            
            class HybridKeywordDenseRerank:
                def __init__(self, corpus: list[str]):
                    self.corpus = corpus
                    # 1. 关键词模型
                    self.kw_vec = TfidfVectorizer(token_pattern=r"(?u)\b\w+\b")
                    self.kw_vec.fit(corpus)
                    # 2. 语义模型
                    self.sem_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                    self.doc_emb = self.sem_model.encode(corpus, normalize_embeddings=True)
            
                def rerank(self, query: str, top_k=3):
                    # 关键词得分
                    q_kw = self.kw_vec.transform([query])
                    doc_kw = self.kw_vec.transform(self.corpus)
                    kw_score = (q_kw * doc_kw.T).toarray().ravel()
                    # 向量得分
                    q_emb = self.sem_model.encode(query, normalize_embeddings=True)
                    vec_score = q_emb @ self.doc_emb.T
                    # 加权融合
                    final = 0.5 * kw_score + 0.5 * vec_score
                    top_idx = np.argsort(final)[-top_k:][::-1]
                    return [(self.corpus[i], float(final[i])) for i in top_idx]
            
            # demo
            rag = HybridKeywordDenseRerank(docs)
            print(rag.rerank("顺丰寄件流程"))
            
      ```
      特点：关键词弥补 OOV 与专有名词，向量兜底语义，比纯向量 baseline 提升 5~15% Hit@3


####  3.高阶版：LLM 自生成关键词 + 同义词扩展 + 图召回
    
        思路：用 1B 小模型离线给每页生成“关键词+同义词”并写入 LanceDB；
             线上同样让 LLM 生成 query 关键词，再图探索/向量检索做召回，最后让同一 LLM 根据摘要生成答案   

        ```
              # 伪代码，完整代码见 [^8^]
              
              from transformers import AutoTokenizer, AutoModelForCausalLM
              import lancedb
              
              KEYWORD_PT = (
                  "Extract up to 10 keywords and their synonyms from the following text. "
                  "Avoid stopwords.\nText: {text}\nKeywords:\n"
              )
              
              class LLMKeywordRAG:
                  def __init__(self, model_name="Llama-3.2-1B-Instruct", table_name="kw_summary"):
                      self.tok = AutoTokenizer.from_pretrained(model_name)
                      self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
                      self.db = lancedb.connect("lancedb")
                      self.table = self.db.open_table(table_name)
              
                  def llm_extract(self, text: str) -> str:
                      prompt = KEYWORD_PT.format(text=text)
                      inputs = self.tok(prompt, return_tensors="pt").to(self.model.device)
                      out = self.model.generate(**inputs, max_new_tokens=64, temperature=0.3)
                      return self.tok.decode(out[0], skip_special_tokens=True).split("Keywords:\n")[-1]
              
                  def index_doc(self, doc: str, doc_id: str):
                      kw = self.llm_extract(doc)
                      summary = self.llm_summary(doc)          # 同模型再生成摘要
                      emb = self.get_embedding(summary)        # 任意嵌入模型
                      self.table.add([{"doc_id": doc_id, "summary": summary, "keywords": kw, "vector": emb}])
              
                  def search(self, query: str, top_k=4):
                      q_kw = self.llm_extract(query)
                      q_emb = self.get_embedding(query)
                      # 1. 向量召回
                      vec_res = self.table.search(q_emb).limit(top_k).to_list()
                      # 2. 关键词重排
                      q_set = set(q_kw.split(","))
                      for r in vec_res:
                          r["kw_score"] = len(set(r["keywords"].split(",")) & q_set)
                      vec_res.sort(key=lambda x: x["kw_score"], reverse=True)
                      return vec_res
              
        ```

      选型小结:
          1.数据量小、延迟要求高 → 方案 1
          2.需要语义但无法上 LLM → 方案 2
          3.文档长、同义词多、预算允许 GPU → 方案 3
         
      把关键词提取当成“可插拔”模块即可，随时与向量、图、SQL 检索组合，不必拘泥于某一种算法。

      
